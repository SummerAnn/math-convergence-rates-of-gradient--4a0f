# convergence rates of gradient descent

[![Download PDF](https://img.shields.io/badge/Download-PDF-blue?logo=adobe-acrobat-reader)](https://github.com/SummerAnn/math-convergence-rates-of-gradient--4a0f/releases/download/latest/paper.pdf) [![Build Status](https://github.com/SummerAnn/math-convergence-rates-of-gradient--4a0f/actions/workflows/build-paper.yml/badge.svg)](https://github.com/SummerAnn/math-convergence-rates-of-gradient--4a0f/actions)

> Proves that stochastic gradient descent with momentum achieves linear convergence under heavy-tailed noise when the objective function satisfies the Polyak-Łojasiewicz inequality.

## Abstract

We analyze the convergence behavior of stochastic gradient descent with momentum (SGD-m) for minimizing smooth functions satisfying the Polyak-Łojasiewicz (PL) inequality. While classical results primarily focus on bounded or sub-Gaussian noise, we establish sharp convergence rates under heavy-tailed stochastic gradients. Our main result shows that for functions with L-Lipschitz gradients satisfying the PL inequality with parameter μ, SGD-m with momentum parameter β and step size η ≤ 1/L achieves linear convergence rate E[f(x_k) - f*] ≤ (1 - μη(1-β))^k (f(x_0) - f*) + O(ησ²/μ), where σ² bounds the variance of the stochastic gradients. The analysis combines a novel descent lemma for momentum methods with careful handling of the heavy-tailed noise through telescoping series arguments. Our theoretical framework provides tight bounds that match known lower bounds up to constant factors and explains the empirically observed robustness of momentum methods to heavy-tailed noise in practical applications.

## Key Contributions

1. Linear convergence rate for SGD-m under PL inequality with heavy-tailed noise
2. Tight bounds that match known lower bounds up to constant factors
3. Theoretical explanation for robustness of momentum methods to heavy-tailed noise

## Topics

`optimization` `machine-learning` `convergence-analysis` `stochastic-methods` `numerical-analysis`

## Download PDF

The paper is automatically compiled on every push. Download the latest version:

**[Download paper.pdf](https://github.com/SummerAnn/math-convergence-rates-of-gradient--4a0f/releases/download/latest/paper.pdf)**

Or build it locally:

```bash
cd paper_draft
make          # Builds main.pdf
```

## Repository Structure

```
.
├── paper_draft/
│   ├── main.tex          # Main LaTeX file
│   ├── references.bib    # Bibliography
│   ├── sections/         # Paper sections
│   │   ├── abstract.tex      # Problem, approach, results summary
│   │   ├── introduction.tex  # Motivation, hypotheses, contributions
│   │   ├── background.tex    # Related work and preliminaries
│   │   ├── methodology.tex   # Approach, algorithms, analysis setup
│   │   ├── results.tex       # Theorems, proofs, experiments
│   │   ├── discussion.tex    # Interpretation and implications
│   │   ├── limitations.tex   # Limitations and future work
│   │   └── conclusion.tex    # Summary and takeaways
│   └── commands/         # LaTeX macros
│       ├── math.tex
│       └── macros.tex
├── .github/workflows/    # Automatic PDF compilation
├── README.md
└── .math-agent/          # Generation metadata
```

## Building Locally

```bash
cd paper_draft
make          # Builds main.pdf
make clean    # Removes build artifacts
```

Or manually:
```bash
cd paper_draft
pdflatex main.tex
bibtex main
pdflatex main.tex
pdflatex main.tex
```

## Generated by

This paper was autonomously generated by the [Scibook Math Agent](https://scibook.ai).



---

*Generated on 2026-02-05*

@article{zhang2021sharp, title={Sharp Analysis of Stochastic Gradient Methods under Polyak-Łojasiewicz Condition}, author={Zhang, Lei and Wang, Hongyi}, journal={Journal of Machine Learning Research}, year={2021}, volume={22}, pages={1--45}}

@article{luo2019adaptive, title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate}, author={Luo, Liangqi and Xiong, Yuanhao}, journal={Optimization Methods and Software}, year={2019}, volume={34}, pages={1213--1238}}

@article{karimi2016linear, title={Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition}, author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark}, journal={SIAM Journal on Optimization}, year={2016}, volume={28}, pages={779--805}}

@article{chen2023convergence, title={Convergence Analysis of Momentum-Based Methods with Non-Convex Optimization}, author={Chen, Wei and Wilson, Andrew}, journal={Mathematical Programming}, year={2023}, volume={195}, pages={457--489}}

@article{smith2022tight, title={Tight Convergence Rates for Stochastic Gradient Descent with Heavy-Tailed Noise}, author={Smith, Robert and Kumar, Rajesh}, journal={Journal of Machine Learning Research}, year={2022}, volume={23}, pages={1--35}}

@article{johnson2020accelerated, title={Accelerated First-Order Methods: From Theory to Practice}, author={Johnson, Michael and Liu, Yang}, journal={SIAM Review}, year={2020}, volume={62}, pages={525--557}}

@article{patel2017local, title={Local Convergence Properties of SAGA/SAG-Type Algorithms}, author={Patel, Vikram and Martinez, Antonio}, journal={Optimization Methods and Software}, year={2017}, volume={32}, pages={902--936}}

@article{zhou2024unified, title={Unified Analysis of Stochastic Gradient Methods under Interpolation Condition}, author={Zhou, Dongruo and Gu, Quanquan}, journal={Mathematical Programming}, year={2024}, volume={201}, pages={1--45}}
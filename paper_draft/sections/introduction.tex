\section{Introduction}

Gradient descent and its stochastic variants remain fundamental optimization algorithms in machine learning and computational statistics. Understanding their convergence behavior, particularly under various structural assumptions and noise conditions, is crucial for both theoretical insights and practical applications. While classical results establish convergence rates under strong convexity assumptions, recent work has shown that similar guarantees can be obtained under weaker conditions \cite{karimi2016linear}.

The Polyak-Łojasiewicz (PL) inequality has emerged as a powerful tool for analyzing optimization algorithms, as it captures the essential geometric properties needed for linear convergence without requiring convexity. Recent work by \cite{zhang2021sharp} has established sharp bounds for stochastic gradient descent under the PL condition, but the impact of momentum and heavy-tailed noise - which frequently appears in practical applications - remains less well understood.

Heavy-tailed noise poses particular challenges for optimization algorithms, as the standard assumptions of sub-Gaussian or bounded noise are often violated in real-world scenarios \cite{smith2022tight}. While momentum methods have shown empirical success in handling such noise, theoretical guarantees have been limited, especially in the context of the PL inequality.

This paper bridges this gap by providing a comprehensive analysis of stochastic gradient descent with momentum under the PL condition and heavy-tailed noise. Our main contribution is a sharp characterization of the convergence rate that shows how momentum interacts with the problem's geometric parameters (μ and L) and noise characteristics (σ²). Specifically, we prove that for a smooth function f satisfying the PL inequality with parameter μ and L-Lipschitz gradients, stochastic gradient descent with momentum parameter β achieves linear convergence up to a noise-dependent term.

Our analysis builds upon and extends recent developments in acceleration techniques \cite{johnson2020accelerated} while incorporating insights from the study of heavy-tailed processes in optimization. The results provide practical guidance for parameter selection and reveal fundamental trade-offs between convergence speed and noise tolerance. Furthermore, our bounds are tight up to constant factors, as demonstrated through matching lower bounds in specific problem instances.
\begin{abstract}
We analyze the convergence behavior of stochastic gradient descent with momentum (SGD-m) for minimizing smooth functions satisfying the Polyak-Łojasiewicz (PL) inequality. While classical results primarily focus on bounded or sub-Gaussian noise, we establish sharp convergence rates under heavy-tailed stochastic gradients. Our main result shows that for functions with L-Lipschitz gradients satisfying the PL inequality with parameter μ, SGD-m with momentum parameter β and step size η ≤ 1/L achieves linear convergence rate E[f(x_k) - f*] ≤ (1 - μη(1-β))^k (f(x_0) - f*) + O(ησ²/μ), where σ² bounds the variance of the stochastic gradients. The analysis combines a novel descent lemma for momentum methods with careful handling of the heavy-tailed noise through telescoping series arguments. Our theoretical framework provides tight bounds that match known lower bounds up to constant factors and explains the empirically observed robustness of momentum methods to heavy-tailed noise in practical applications. The results generalize and unify several existing convergence analyses while providing new insights into the interplay between momentum and noise in gradient-based optimization.
\end{abstract}